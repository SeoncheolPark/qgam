% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tuneLearn.R
\name{tuneLearn}
\alias{tuneLearn}
\title{Tuning the learning rate for Gibbs posterior}
\usage{
tuneLearn(form, data, lsig, qu, err = 0.05, multicore = !is.null(cluster),
  cluster = NULL, ncores = detectCores() - 1, paropts = list(),
  control = list(), argGam = NULL)
}
\arguments{
\item{form}{A GAM formula, or a list of formulae. See ?mgcv::gam details.}

\item{data}{A data frame or list containing the model response variable and covariates required by the formula.
By default the variables are taken from environment(formula): typically the environment from which gam is called.}

\item{lsig}{A vector of value of the log learning rate (log(sigma)) over which the calibration loss function is evaluated.}

\item{qu}{The quantile of interest. Should be in (0, 1).}

\item{err}{An upper bound on the error of the estimated quantile curve. Should be in (0, 1). See Fasiolo et al. (2016) for details.}

\item{multicore}{If TRUE the calibration will happen in parallel.}

\item{cluster}{An object of class \code{c("SOCKcluster", "cluster")}. This allowes the user to pass her own cluster,
which will be used if \code{multicore == TRUE}. The user has to remember to stop the cluster.}

\item{ncores}{Number of cores used. Relevant if \code{multicore == TRUE}.}

\item{paropts}{a list of additional options passed into the foreach function when parallel computation is enabled. 
This is important if (for example) your code relies on external data or packages: 
use the .export and .packages arguments to supply them so that all cluster nodes 
have the correct environment set up for computing.}

\item{control}{A list of control parameters for \code{tuneLearn} with entries: \itemize{
                  \item{\code{K} = number of boostrap datasets used for calibration. By default \code{K=50}.}
                  \item{\code{b} = offset parameter used by the mgcv::gauslss. By default \code{b=0}.}
                  \item{\code{verbose} = if TRUE some more details are given. By default \code{verbose=FALSE}.}
                  \item{\code{progress} = argument passed to plyr::llply. By default \code{progress="text"} so that progress
                                          is reported. Set it to \code{"none"} to avoid it.}
}}

\item{argGam}{A list of parameters to be passed to \code{mgcv::gam}. This list can potentially include all the arguments listed
in \code{?gam}, with the exception of \code{formula}, \code{family} and \code{data}.}
}
\value{
A list with entries: \itemize{
                  \item{\code{lsig} = the value of log(sigma) resulting in the lowest loss.}
                  \item{\code{loss} = vector containing the value of the calibration loss function corresponding 
                                      to each value of log(sigma).}
                  \item{\code{edf} = a matrix where the first colums contain the log(sigma) sequence, and the remaining
                                     columns contain the corresponding effective degrees of freedom of each smooth.}
                  \item{\code{convProb} = a logical vector indicating, for each value of log(sigma), whether the outer
                                          optimization which estimates the smoothing parameters has encountered convergence issues.
                                          \code{FALSE} means no problem.}
                                          }
}
\description{
The learning rate (sigma) of the Gibbs posterior is tuned using a calibration approach,
             based on boostrapping. Here the calibration loss function is evaluated on a grid of values
             provided by the user.
}
\examples{
library(qgam); library(MASS)

# Calibrate learning rate on a grid
set.seed(41444)
sigSeq <- seq(1.5, 5, length.out = 10)
closs <- tuneLearn(form = accel~s(times,k=20,bs="ad"), 
                   data = mcycle, 
                   err = 0.05, 
                   lsig = sigSeq, 
                   qu = 0.5)

plot(sigSeq, closs$loss, type = "b", ylab = "Calibration Loss", xlab = "log(sigma)")

# Pick best log-sigma
best <- sigSeq[ which.min(closs$loss) ]
abline(v = best, lty = 2)

# Fit using the best sigma
fit <- qgam(accel~s(times,k=20,bs="ad"), data = mcycle, qu = 0.5, err = 0.05, lsig = best)
summary(fit)

pred <- predict(fit, se=TRUE)
plot(mcycle$times, mcycle$accel, xlab = "Times", ylab = "Acceleration", 
     ylim = c(-150, 80))
lines(mcycle$times, pred$fit, lwd = 1)
lines(mcycle$times, pred$fit + 2*pred$se.fit, lwd = 1, col = 2)
lines(mcycle$times, pred$fit - 2*pred$se.fit, lwd = 1, col = 2)                        
}
\author{
Matteo Fasiolo <matteo.fasiolo@gmail.com>.
}
\references{
Fasiolo, M., Goude, Y., Nedellec, R. and Wood, S. N. (2016). Fast calibrated additive quantile regression. Available at
            \url{https://github.com/mfasiolo/qgam/draft_qgam.pdf}.
}

